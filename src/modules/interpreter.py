import logging
import os

import openai
from tenacity import retry, stop_after_attempt, wait_random_exponential

logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] [%(process)d] [%(levelname)s] [%(filename)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S %z",
)
logging.getLogger("openai").setLevel(logging.WARNING)


@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def get_completion(command: str, context: str, config: dict):
    """
    This function uses the OpenAI API to generate a completion for a given
    prompt. The prompt is generated by inserting the value of context
    into the template_prompt string. The temperature and maximum number of
    tokens can be controlled through the temp and max_t arguments, respectively.
    The model argument specifies which OpenAI model to use for generating the
    completion. The function returns the generated completion as a string.
    """
    openai.api_key = os.environ["OPENAI_API_KEY"]
    logging.info(f"Requesting OpenAI for {command}")
    model = config["completions_engine"]
    template_prompt, max_t, temp = (
        config["prompts"][command]["prompt"],
        config["prompts"][command]["max_tokens"],
        config["prompts"][command]["temperature"],
    )
    formatted_prompt = template_prompt.format(text=context)
    completions = openai.Completion.create(
        engine=model,
        prompt=formatted_prompt,
        max_tokens=max_t,
        n=1,
        temperature=temp,
        top_p=1,
    )
    top_completion = completions.choices[0]
    usage = completions["usage"]
    logging.info(f"Total usage: {usage['total_tokens']} tokens")
    logging.info(f"Prompt: {usage['prompt_tokens']} tokens")
    logging.info(f"Completion: {usage['completion_tokens']} tokens")
    return top_completion.text
